{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428faf67",
   "metadata": {},
   "source": [
    "### 1. How does unsqueeze help us to solve certain broadcasting problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9539f1",
   "metadata": {},
   "source": [
    "`unsqueeze` in PyTorch adds a new dimension to a tensor, effectively increasing its dimensions by one. This is particularly useful in broadcasting problems when you need to align dimensions for elementwise operations. In very short:\n",
    "\n",
    "- **Broadcasting Alignment**: `unsqueeze` helps align dimensions by adding new dimensions where needed, allowing for consistent broadcasting.\n",
    "\n",
    "- **Avoiding Shape Mismatch**: It prevents shape mismatch errors by adjusting the tensor's shape to match the required broadcasting dimensions.\n",
    "\n",
    "For example, if you have a 1D tensor and need to perform an operation with a 2D tensor, you can use `unsqueeze` to add a dimension to the 1D tensor, making it compatible for broadcasting with the 2D tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72082b6",
   "metadata": {},
   "source": [
    "### 2. How can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc19395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: [1 2 3]\n",
      "Array after expanding: [[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Using indexing to achieve the same as unsqueeze\n",
    "a = np.array([1, 2, 3])  # 1D array\n",
    "a_expanded = a[:, None]  # Adding a new axis\n",
    "\n",
    "print(\"Original array:\", a)\n",
    "print(\"Array after expanding:\", a_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d3b508",
   "metadata": {},
   "source": [
    "### 3. How do we show the actual contents of the memory used for a tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ac5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the memory: [1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_16620\\3267775679.py:7: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  memory_contents = tensor.storage().tolist()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Display the contents of the memory as a list\n",
    "memory_contents = tensor.storage().tolist()\n",
    "\n",
    "print(\"Contents of the memory:\", memory_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc61f42",
   "metadata": {},
   "source": [
    "### 4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce841d7",
   "metadata": {},
   "source": [
    "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each column of the matrix. Each element of the vector is added to the corresponding element in each column of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b9c27",
   "metadata": {},
   "source": [
    "### 5. Do broadcasting and expand_as result in increased memory use? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a340ba",
   "metadata": {},
   "source": [
    "No, broadcasting and `expand_as` do not result in increased memory use. They operate on the original data without creating additional copies. Broadcasting achieves efficient elementwise operations by utilizing existing memory efficiently, and `expand_as` modifies the shape without duplicating the actual data, saving memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ed84b",
   "metadata": {},
   "source": [
    "### 6. Implement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadcfe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using Einstein summation:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matmul_einstein(A, B):\n",
    "    return np.einsum('ij, jk -> ik', A, B)\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "result = matmul_einstein(A, B)\n",
    "print(\"Matrix multiplication using Einstein summation:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d448266",
   "metadata": {},
   "source": [
    "### 7. What does a repeated index letter represent on the lefthand side of einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd1e06",
   "metadata": {},
   "source": [
    "A repeated index letter on the left-hand side of `einsum` in the Einstein summation notation represents summation over that index. For example, 'ii' represents summation over the same index 'i', typically found in diagonal elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b90a94",
   "metadata": {},
   "source": [
    "### 8. What are the three rules of Einstein summation notation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a90be4",
   "metadata": {},
   "source": [
    "The three rules of Einstein summation notation, in very short:\n",
    "\n",
    "1. **Repeating Indices**: Repeating an index implies summation over that index.\n",
    "\n",
    "2. **Free Indices**: Any index that appears only once on the left-hand side and once on the right-hand side is a free index and remains unchanged.\n",
    "\n",
    "3. **Matching Indices**: Matching indices on both sides imply multiplication of the corresponding elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f444f0",
   "metadata": {},
   "source": [
    "### 9. What are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4b739",
   "metadata": {},
   "source": [
    "- **Forward Pass**: The process of passing input data through the neural network, layer by layer, to compute the output or prediction.\n",
    "\n",
    "- **Backward Pass (Backpropagation)**: The process of computing gradients of the loss with respect to each parameter in the network, starting from the output layer and moving back to the input layer. These gradients are then used to update the weights through optimization algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b29c0a",
   "metadata": {},
   "source": [
    "### 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e03f89",
   "metadata": {},
   "source": [
    "Storing activations from intermediate layers in the forward pass is necessary to use them later during the backward pass (backpropagation). These activations are crucial for calculating gradients accurately, which enables efficient weight updates during training. Without storing them, we would lose the necessary information to compute gradients accurately and update the model parameters properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a178c13",
   "metadata": {},
   "source": [
    "### 11. What is the downside of having activations with a standard deviation too far away from 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbec06",
   "metadata": {},
   "source": [
    "Having activations with a standard deviation too far from 1 in a neural network can slow down or destabilize the training process. Extreme values can lead to issues like vanishing or exploding gradients, making it harder for the network to learn effectively and converge during training. Maintaining activations with a reasonable standard deviation, such as around 1, helps in stabilizing and accelerating the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf05345",
   "metadata": {},
   "source": [
    "### 12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085e823",
   "metadata": {},
   "source": [
    "Proper weight initialization helps avoid activation standard deviation issues by setting initial weights in a way that balances the signal flow in forward and backward passes. Methods like He initialization or Xavier/Glorot initialization ensure that activations neither vanish nor explode, promoting stable training and faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
