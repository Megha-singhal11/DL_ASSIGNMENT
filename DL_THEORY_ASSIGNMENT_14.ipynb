{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82afaacf",
   "metadata": {},
   "source": [
    "### 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896f332",
   "metadata": {},
   "source": [
    "Yes, it is generally okay to initialize all weights to the same value as long as that value is selected randomly using He initialization. This approach helps to maintain symmetry breaking and encourage effective learning during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5ccad",
   "metadata": {},
   "source": [
    "### 2. Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972867ea",
   "metadata": {},
   "source": [
    "Yes, it is generally okay to initialize the bias terms to 0. This is a common practice and often works well in many scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e02e7",
   "metadata": {},
   "source": [
    "### 3. Name three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff0af2",
   "metadata": {},
   "source": [
    "1. **Smoothness**: ELU is smooth everywhere, including around zero, which helps in smoother gradients and stable learning.\n",
    "\n",
    "2. **Handles Negative Inputs**: ELU can handle negative inputs, preventing dead neurons and potentially improving model performance.\n",
    "\n",
    "3. **Closer to Zero Mean Outputs**: ELU tends to push the mean of the activations closer to zero, which can aid in model convergence and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7292a",
   "metadata": {},
   "source": [
    "### 4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f8f10",
   "metadata": {},
   "source": [
    "- **ELU**: Use ELU when you want a smooth activation function that handles negative inputs and can speed up learning while reducing the likelihood of dead neurons.\n",
    "  \n",
    "- **Leaky ReLU (and variants)**: Leaky ReLU is useful when you want to alleviate the dying ReLU problem by allowing a small, positive gradient for negative inputs. Variants like Parametric ReLU and Randomized Leaky ReLU can be used to adapt the leaky behavior.\n",
    "\n",
    "- **ReLU**: ReLU is a good default choice due to its simplicity and computational efficiency. Use it when you want a computationally efficient activation function with faster convergence, especially for deep networks.\n",
    "\n",
    "- **Tanh**: Use tanh when you need the output in the range [-1, 1]. It is commonly used in the hidden layers of models.\n",
    "\n",
    "- **Logistic (Sigmoid)**: Use logistic (sigmoid) in binary classification problems for the output layer to get probabilities that sum up to 1.\n",
    "\n",
    "- **Softmax**: Use softmax in multi-class classification problems for the output layer to get probabilities for each class that sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d21bc",
   "metadata": {},
   "source": [
    "### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe3626",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) can lead to oscillations in the optimization process, causing the optimizer to overshoot the optimal solution and potentially diverge, making convergence unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d10ff1",
   "metadata": {},
   "source": [
    "### 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cfe60",
   "metadata": {},
   "source": [
    "1. **Pruning**: Remove connections (weights or neurons) with small magnitudes during or after training to create a sparse model.\n",
    "\n",
    "2. **L1 Regularization**: Use L1 regularization in the loss function to encourage sparsity by driving many weights to exactly zero during training.\n",
    "\n",
    "3. **Quantization**: Quantize weights to a smaller set of discrete values, reducing the precision and creating a sparse representation with zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e0070",
   "metadata": {},
   "source": [
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f13f2f",
   "metadata": {},
   "source": [
    "Yes, Dropout can slow down training because it introduces additional computations during each training step. However, during inference, Dropout does not slow down significantly; it's simply turned off, so it doesn't affect prediction speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
