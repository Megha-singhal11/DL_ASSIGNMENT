{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3863e40a",
   "metadata": {},
   "source": [
    "### 1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0308a",
   "metadata": {},
   "source": [
    "1. **Sequence-to-Sequence RNN**:\n",
    "   - **Language Translation**: Translate text from one language to another.\n",
    "   - **Chatbot Conversations**: Generate meaningful responses in a conversation.\n",
    "   - **Summarization**: Condense a long text into a shorter, meaningful summary.\n",
    "   - **Speech Recognition and Synthesis**: Convert spoken language into written text or vice versa.\n",
    "\n",
    "2. **Sequence-to-Vector RNN**:\n",
    "   - **Sentiment Analysis**: Analyze a text and output a sentiment score (e.g., positive, negative, neutral).\n",
    "   - **Document Classification**: Classify a document into predefined categories.\n",
    "   - **Question Answering**: Answer questions based on a given context or document.\n",
    "   - **Image Captioning**: Describe an image in natural language using a fixed-length vector representation.\n",
    "\n",
    "3. **Vector-to-Sequence RNN**:\n",
    "   - **Music Generation**: Create a musical composition based on an initial seed or style.\n",
    "   - **Image Generation from Text**: Generate an image from a textual description.\n",
    "   - **Text Generation**: Generate a sequence of text (e.g., sentences, paragraphs) based on an initial vector input.\n",
    "   - **Speech Synthesis from Text**: Generate speech from a given textual input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9e42a",
   "metadata": {},
   "source": [
    "### 2. How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15544f00",
   "metadata": {},
   "source": [
    "The input to an RNN layer typically has three dimensions:\n",
    "- **Batch Size**: Number of sequences processed in a single batch.\n",
    "- **Sequence Length**: Length of each sequence in terms of time steps or tokens.\n",
    "- **Features/Inputs per Time Step**: Dimension representing the features or inputs at each time step.\n",
    "\n",
    "The output of an RNN layer also typically has three dimensions:\n",
    "- **Batch Size**: Same as the input, representing the number of sequences in a batch.\n",
    "- **Sequence Length**: Represents the length of the output sequence.\n",
    "- **Features/Outputs per Time Step**: Dimension representing the features or outputs at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3cf47",
   "metadata": {},
   "source": [
    "### 3. If you want to build a deep sequence-to-sequence RNN, which RNN layers should have return_sequences=True? What about a sequence-to-vector RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f2205",
   "metadata": {},
   "source": [
    "In a deep sequence-to-sequence RNN, all intermediate RNN layers should have `return_sequences=True`. This allows each RNN layer to output sequences, which is essential for the subsequent RNN layers to receive sequences as input.\n",
    "\n",
    "For a sequence-to-vector RNN (where the goal is to produce a fixed-length vector representation for the entire input sequence):\n",
    "- The final RNN layer (at the encoder side) should have `return_sequences=False` to produce a single vector representing the entire input sequence.\n",
    "- The decoder side can have all RNN layers with `return_sequences=True` until the final layer, which should have `return_sequences=False` to output a single vector for the entire generated sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2da49f",
   "metadata": {},
   "source": [
    "### 4. Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2506b00",
   "metadata": {},
   "source": [
    "For forecasting the next seven days in a daily univariate time series, you can use a simple RNN architecture with the following configuration:\n",
    "- Single-layer RNN with sufficient units.\n",
    "- Input sequence length corresponding to the historical data (e.g., past 30 days).\n",
    "- Output sequence length of 7 to predict the next seven days.\n",
    "- Train the RNN to learn the mapping from the historical input sequence to the next 7-day output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976b9d5",
   "metadata": {},
   "source": [
    "### 5. What are the main difficulties when training RNNs? How can you handle them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92341b",
   "metadata": {},
   "source": [
    "Main difficulties when training RNNs:\n",
    "1. **Vanishing/Exploding Gradients**: Address using gradient clipping or advanced activation functions like LSTM or GRU.\n",
    "   \n",
    "2. **Long-Term Dependencies**: Mitigate using LSTM or GRU cells that allow information to persist and selective forgetting.\n",
    "\n",
    "3. **Overfitting**: Combat by using dropout, early stopping, and regularization techniques.\n",
    "\n",
    "4. **Choosing the Right Architecture**: Experiment with various architectures and hyperparameters to find the most effective one for the specific task.\n",
    "\n",
    "5. **Training Speed and Efficiency**: Optimize training using batch normalization, efficient activation functions, and optimized libraries.\n",
    "\n",
    "6. **Data Preprocessing**: Properly preprocess data, including scaling, normalization, and handling missing values.\n",
    "\n",
    "7. **Hyperparameter Tuning**: Use techniques like grid search or random search to find the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfc1ac",
   "metadata": {},
   "source": [
    "### 6. Can you sketch the LSTM cell’s architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df80b18",
   "metadata": {},
   "source": [
    "Yes, we can sketch the LSTM cell's architecture.\n",
    "The LSTM cell comprises three main gates:\n",
    "- **Forget Gate**: Decides what information to discard from the cell state.\n",
    "- **Input Gate**: Determines what new information to store in the cell state.\n",
    "- **Output Gate**: Controls what information to output to the next layer.\n",
    "\n",
    "These gates use sigmoid and tanh activation functions to regulate the flow of information, making LSTM capable of handling long-term dependencies. The cell state flows through these gates, and relevant information is selectively stored, forgotten, and outputted based on gate activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a72ea",
   "metadata": {},
   "source": [
    "### 7. Why would you want to use 1D convolutional layers in an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965c6d8",
   "metadata": {},
   "source": [
    "Using 1D convolutional layers in an RNN can help capture local patterns and features within the sequence efficiently. It enables the RNN model to learn hierarchical representations of the data by identifying short-term patterns before processing them in the RNN layers, potentially improving the model's ability to extract meaningful features and relationships from the sequential data. This can enhance performance and speed up training in certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567c4a1",
   "metadata": {},
   "source": [
    "### 8. Which neural network architecture could you use to classify videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abb166",
   "metadata": {},
   "source": [
    "For video classification:\n",
    "- **Convolutional Neural Network (CNN)**: Effective in extracting spatial features from video frames.\n",
    "- **Recurrent Neural Network (RNN)**: Useful for capturing temporal dependencies over time.\n",
    "- **3D Convolutional Neural Network (3D CNN)**: Integrates spatial and temporal features for video understanding.\n",
    "- **Convolutional Recurrent Neural Network (CRNN)**: Combines CNN for spatial features and RNN for temporal modeling.\n",
    "- **Transformer-based Models**: Effective for processing sequences of data, including videos, by attending to different parts of the frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b50a79",
   "metadata": {},
   "source": [
    "### 9. Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd68dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b674ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the SketchRNN dataset\n",
    "tfds.load\n",
    "# Assuming X_train, y_train, X_test, y_test are loaded and preprocessed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the SketchRNN dataset (replace with your actual data loading)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data and normalize pixel values to [0, 1]\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the images to a single channel (grayscale)\n",
    "X_train_full = X_train_full.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes for MNIST digits\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)  # Adjust num_classes based on your dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
