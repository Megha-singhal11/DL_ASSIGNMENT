{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ae3725",
   "metadata": {},
   "source": [
    "### 1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f5d13",
   "metadata": {},
   "source": [
    "a) **Sigmoid**: Sigmoid squashes any input to be between 0 and 1. It's like a switch that turns things on or off, making it useful for binary classification problems.\n",
    "\n",
    "b) **Tanh (Hyperbolic Tangent)**: Tanh squashes input between -1 and 1. It's similar to sigmoid but centered around zero, useful for outputs that can be positive or negative.\n",
    "\n",
    "c) **ReLU (Rectified Linear Unit)**: ReLU is like a light switch; if the input is positive, it lets it through as is, but if it's negative, it turns it off completely. It's widely used for faster training in deep neural networks.\n",
    "\n",
    "d) **ELU (Exponential Linear Unit)**: ELU is similar to ReLU for positive values, but for negative values, it tapers off smoothly instead of abruptly turning off. It helps prevent dead neurons and allows for negative values.\n",
    "\n",
    "e) **LeakyReLU**: LeakyReLU is a variation of ReLU that allows a small, positive output for negative inputs, preventing neurons from dying and aiding in convergence.\n",
    "\n",
    "f) **Swish**: Swish is a smoothly transitioning activation function that tends to work well in practice. It's like a sigmoid that's 'turned on' by the input value, providing a more gradual activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8992535",
   "metadata": {},
   "source": [
    "### 2. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c4c74",
   "metadata": {},
   "source": [
    "- **Increase Learning Rate**: Faster convergence but may overshoot the optimal point, leading to instability or divergence.\n",
    "\n",
    "- **Decrease Learning Rate**: Smoother convergence, but slower training as smaller steps are taken towards the optimal point, reducing the risk of overshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c161e",
   "metadata": {},
   "source": [
    "### 3. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe396bc5",
   "metadata": {},
   "source": [
    "Increasing the number of internal hidden neurons can lead to a more complex model with higher capacity to learn intricate patterns in the data. However, it also raises the risk of overfitting if not regulated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c736a9b",
   "metadata": {},
   "source": [
    "### 4. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f29c0",
   "metadata": {},
   "source": [
    "Increasing the batch size can speed up computation due to parallelism, but it may lead to a loss in model generalization. Smaller batches introduce noise, which can help the model escape local minima and generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b288880",
   "metadata": {},
   "source": [
    "### 5. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd687da",
   "metadata": {},
   "source": [
    "Regularization helps avoid overfitting by adding a penalty to the model's complexity, discouraging overly complex patterns in the data. It promotes simpler models, making them more likely to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098e062",
   "metadata": {},
   "source": [
    "### 6. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad65a62",
   "metadata": {},
   "source": [
    "- **Loss Function**: Measures the error between predicted values and true values for a single training example. It guides the model to improve during training.\n",
    "\n",
    "- **Cost Function**: Aggregates the losses across all training examples. It's the average of the losses and provides a single scalar value to optimize during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c12aa8",
   "metadata": {},
   "source": [
    "### 7. What do ou mean by underfitting in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0e412",
   "metadata": {},
   "source": [
    "Underfitting in neural networks occurs when the model is too simple to capture the underlying patterns in the data. It performs poorly not only on the training data but also on unseen or new data. The model fails to learn the complexities and nuances present in the dataset, resulting in suboptimal performance and an inability to generalize effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56917f0c",
   "metadata": {},
   "source": [
    "### 8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c155d3",
   "metadata": {},
   "source": [
    "Dropout is used in neural networks to prevent overfitting and improve generalization. It works by randomly setting a fraction of the neurons in a layer to zero during each training batch. This effectively drops out some connections, forcing the network to learn more robust and generalized features. Dropout acts as a regularization technique, encouraging the network to not rely too heavily on any specific neuron, thus reducing the risk of overfitting and enhancing the model's ability to generalize to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
